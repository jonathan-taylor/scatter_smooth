{"version":"1","records":[{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline"},"type":"lvl1","url":"/compare-r","position":0},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline"},"content":"This document demonstrates the usage of the smoothing_spline package and compares it with the standard smooth.spline function in R. We will use the Bikeshare dataset from the ISLP package.","type":"content","url":"/compare-r","position":1},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Setup"},"type":"lvl2","url":"/compare-r#setup","position":2},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Setup"},"content":"First, we need to import the necessary libraries and load the rpy2 extension to run R code directly in this notebook.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nfrom ISLP import load_data\nfrom smoothing_spline import SplineFitter\n\n%load_ext rpy2.ipython\n\n\n\n","type":"content","url":"/compare-r#setup","position":3},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Loading the Data"},"type":"lvl2","url":"/compare-r#loading-the-data","position":4},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Loading the Data"},"content":"We will use the Bikeshare dataset, which contains daily counts of bike rentals in Washington D.C.\n\nBike = load_data('Bikeshare')\nBike.head()\n\n\n\nWe will focus on the relationship between the hour of the day (hr) and the number of bikers (bikers). Since hr is categorical in the original dataset but represents time, we convert it to numeric.\n\ndf = 7\n# 'bikers' is 'cnt' in the original dataset, ISLP might have renamed it or we use 'cnt'\nif 'bikers' not in Bike.columns:\n    Bike['bikers'] = Bike['cnt']\n\nhr_numeric = pd.to_numeric(Bike['hr'])\nbikers = Bike['bikers']\n\n# Sort by hour for cleaner plotting lines\nsorted_idx = np.argsort(hr_numeric)\nx_plot = hr_numeric.iloc[sorted_idx].unique()\n\n\n\n","type":"content","url":"/compare-r#loading-the-data","position":5},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Fitting Smoothing Splines"},"type":"lvl2","url":"/compare-r#fitting-smoothing-splines","position":6},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Fitting Smoothing Splines"},"content":"","type":"content","url":"/compare-r#fitting-smoothing-splines","position":7},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl3":"1. Using smoothing_spline (Python)","lvl2":"Fitting Smoothing Splines"},"type":"lvl3","url":"/compare-r#id-1-using-smoothing-spline-python","position":8},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl3":"1. Using smoothing_spline (Python)","lvl2":"Fitting Smoothing Splines"},"content":"We fit a smoothing spline with a specified degrees of freedom (df=5).\n\n# Fit model\nspl_py = SplineFitter(x=hr_numeric, df=df)\nspl_py.fit(bikers)\n\n# Predict\ny_py = spl_py.predict(x_plot)\n\n\n\n","type":"content","url":"/compare-r#id-1-using-smoothing-spline-python","position":9},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl3":"2. Using smooth.spline (R)","lvl2":"Fitting Smoothing Splines"},"type":"lvl3","url":"/compare-r#id-2-using-smooth-spline-r","position":10},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl3":"2. Using smooth.spline (R)","lvl2":"Fitting Smoothing Splines"},"content":"We fit the same model using R. We transfer the data to R and run the smooth.spline function.\n\n%%R -i hr_numeric -i bikers -o y_r -i df\n# Fit model in R\nfit_r <- smooth.spline(hr_numeric, bikers, df=df)\n\n# Predict at unique hours\n# unique() in R returns unsorted, but we want to match x_plot order\nx_vals <- sort(unique(hr_numeric))\npred_r <- predict(fit_r, x_vals)\ny_r <- pred_r$y\n\n\n\n","type":"content","url":"/compare-r#id-2-using-smooth-spline-r","position":11},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl3":"Comparison","lvl2":"Fitting Smoothing Splines"},"type":"lvl3","url":"/compare-r#comparison","position":12},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl3":"Comparison","lvl2":"Fitting Smoothing Splines"},"content":"Let’s visualize the results. They should be nearly identical.\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(hr_numeric + np.random.normal(0, 0.1, len(hr_numeric)), bikers, \n            s=1, c='lightgray', alpha=0.5, label='Data')\nax.plot(x_plot, y_py, 'b-', lw=3, label='Python (smoothing_spline)', alpha=0.8)\nax.plot(x_plot, y_r, 'r--', lw=3, label='R (smooth.spline)', alpha=0.8)\nax.set_xlabel(\"Hour\")\nax.set_ylabel(\"Number of Bikers\")\nax.set_title(\"Comparison of Smoothing Splines (df={df})\")\nax.legend()\nplt.show()\n\n# Numerical comparison\n# Note: R might handle repeated x values slightly differently (using weights)\n# smoothing_spline handles them naturally in the basis construction.\ndiff = np.mean(np.abs(y_py - y_r))\nprint(f\"Mean Absolute Difference: {diff:.6f}\")\n\n\n\n\n\n","type":"content","url":"/compare-r#comparison","position":13},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Speed Comparison"},"type":"lvl2","url":"/compare-r#speed-comparison","position":14},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Speed Comparison"},"content":"We will compare the execution time for fitting the model.\n\n# Python Timing\nt_py = %timeit -o -n 10 -r 3 SplineFitter(x=hr_numeric, df=10).fit(bikers)\n\n# R Timing\n\n\n\n%%R -i hr_numeric -i bikers\nlibrary(microbenchmark)\n# R Timing\nmicrobenchmark(\n  smooth.spline(hr_numeric, bikers, df=10),\n  times=10\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/compare-r#speed-comparison","position":15},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Automatic Tuning with GCV"},"type":"lvl2","url":"/compare-r#automatic-tuning-with-gcv","position":16},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Automatic Tuning with GCV"},"content":"One of the key features of smoothing splines is the automatic selection of the smoothing parameter (\\lambda) using Generalized Cross-Validation (GCV).","type":"content","url":"/compare-r#automatic-tuning-with-gcv","position":17},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl3":"In R","lvl2":"Automatic Tuning with GCV"},"type":"lvl3","url":"/compare-r#in-r","position":18},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl3":"In R","lvl2":"Automatic Tuning with GCV"},"content":"R’s smooth.spline uses GCV (or CV) by default if df and spar are not specified.\n\n%%R\nfit_gcv <- smooth.spline(hr_numeric, bikers, cv=FALSE) # cv=FALSE implies GCV\ncat(\"Selected df (R):\", fit_gcv$df, \"\n\")\ncat(\"Selected lambda (R):\", fit_gcv$lambda, \"\n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/compare-r#in-r","position":19},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl3":"In Python (smoothing_spline)","lvl2":"Automatic Tuning with GCV"},"type":"lvl3","url":"/compare-r#in-python-smoothing-spline","position":20},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl3":"In Python (smoothing_spline)","lvl2":"Automatic Tuning with GCV"},"content":"The smoothing_spline package also supports finding \\lambda that minimizes the GCV score via the solve_gcv method.\n\n# Initialize fitter with data\n# Note: We use the internal C++ fitter for speed if available\nfitter = SplineFitter(x=hr_numeric, knots=np.unique(hr_numeric))\n\n# Solve for GCV\nbest_lam = fitter.solve_gcv(bikers)\nprint(f\"Selected lambda (Python): {best_lam}\")\n\n# Get corresponding df\n# We can access the internal fitter to compute DF for verification\nif fitter._cpp_fitter:\n    best_lam_scaled = best_lam / fitter.x_scale_**3\n    best_df = fitter._cpp_fitter.compute_df(best_lam_scaled)\n    print(f\"Selected df (Python): {best_df}\")\nelse:\n    best_df = \"N/A (C++ extension not available)\"\n\n\n\nWe can now visualize the optimal fit.\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(hr_numeric + np.random.normal(0, 0.1, len(hr_numeric)), bikers, \n            s=1, c='lightgray')\n# fitter is already fitted with best_lam by solve_gcv\nax.plot(x_plot, fitter.predict(x_plot), 'g-', lw=3, label=f'Optimal GCV (df={best_df:.2f})')\nax.set_xlabel(\"Hour\")\nax.set_ylabel(\"Number of Bikers\")\nax.legend()\nplt.show()\n\n\n\n","type":"content","url":"/compare-r#in-python-smoothing-spline","position":21},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Another Example: Log-Transformation"},"type":"lvl2","url":"/compare-r#another-example-log-transformation","position":22},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Another Example: Log-Transformation"},"content":"The relationship between hours and bikers might be better modeled on a log scale, as counts are non-negative and variance often increases with the mean.\n\n# Fit model on log(bikers)\nlog_bikers = np.log(bikers + 1) # Add 1 to avoid log(0)\nspl_log = SplineFitter(x=hr_numeric, df=df)\nspl_log.fit(log_bikers)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(hr_numeric + np.random.normal(0, 0.1, len(hr_numeric)), log_bikers, \n            s=1, c='lightgray')\nax.plot(x_plot, spl_log.predict(x_plot), 'purple', lw=3, label='Log-Smoothing Spline (df={df})')\nax.set_xlabel(\"Hour\")\nax.set_ylabel(\"Log(Number of Bikers + 1)\")\nax.legend()\nplt.show()\n\n\n\n","type":"content","url":"/compare-r#another-example-log-transformation","position":23},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Synthetic Speed Comparison (N=500)"},"type":"lvl2","url":"/compare-r#synthetic-speed-comparison-n-500","position":24},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Synthetic Speed Comparison (N=500)"},"content":"Finally, we compare the speed of both implementations on a synthetic dataset with 500 unique x values.\n\n# Synthetic data\nn_syn = 500\nrng = np.random.default_rng(0)\nx_syn = np.sort(rng.uniform(0, 10, n_syn))\ny_syn = np.sin(x_syn) + rng.normal(0, 0.1, n_syn)\n\n# Python Timing (using all unique x as knots)\nprint(f\"Python Timing (n={n_syn}, all knots):\")\n%timeit SplineFitter(x=x_syn, df=10).fit(y_syn)\n\n\n\n\n\n%%R -i x_syn -i y_syn\nlibrary(microbenchmark)\ncat(sprintf(\"R Timing (n=%d):\\n\", length(x_syn)))\n\n# Fit once to inspect knots\nfit_r_syn <- smooth.spline(x_syn, y_syn, df=10, all.knots=TRUE)\nn_knots_r <- fit_r_syn$fit$nk\ncat(sprintf(\"Number of knots used by R: %d\\n\", n_knots_r))\n\nmicrobenchmark(\n  smooth.spline(x_syn, y_syn, df=10),\n  times=100\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/compare-r#synthetic-speed-comparison-n-500","position":25},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Limited knots"},"type":"lvl2","url":"/compare-r#limited-knots","position":26},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Limited knots"},"content":"\n\nR’s smooth.spline automatically reduces the number of knots when N > 49 (typically to around 50-100) to speed up computation. For N=10000, R uses about 200 knots.\n\nTo make a fair speed comparison, we can limit the number of knots in Python as well.\n\n# Python Timing with reduced knots\nn_syn = 10000\nx_syn = np.sort(rng.uniform(0, 10, n_syn))\ny_syn = np.sin(x_syn) + rng.normal(0, 0.1, n_syn)\nn_knots_reduced = 200 # Similar to R's behavior\nprint(f\"Python Timing (n={n_syn}, n_knots={n_knots_reduced}):\")\n%timeit SplineFitter(x=x_syn, df=10, n_knots=n_knots_reduced).fit(y_syn)\n\n\n\n\n\n\n\n%%R -i x_syn -i y_syn\nlibrary(microbenchmark)\ncat(sprintf(\"R Timing (n=%d):\\n\", length(x_syn)))\n\n# Fit once to inspect knots\nfit_r_syn <- smooth.spline(x_syn, y_syn, df=10, nknots=200)\nn_knots_r <- fit_r_syn$fit$nk\ncat(sprintf(\"Number of knots used by R: %d\\n\", n_knots_r))\n\nmicrobenchmark(\n  smooth.spline(x_syn, y_syn, df=10),\n  times=100\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/compare-r#limited-knots","position":27},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Performance Note"},"type":"lvl2","url":"/compare-r#performance-note","position":28},{"hierarchy":{"lvl1":"Comparing smoothing_spline with R’s smooth.spline","lvl2":"Performance Note"},"content":"While the C++ implementation offers significant speedups over the pure Python version, it is currently slower than R’s smooth.spline for exact degrees of freedom calculation. This is because our current implementation of the EDF calculation involves O(N^2) operations (specifically, N back-solves to compute the trace of the inverse), whereas R likely uses a specialized O(N) algorithm for computing the trace of a banded matrix inverse.\n\nFor large N, the Hutchinson estimator (randomized trace estimation) is used to mitigate this cost, reducing the complexity to O(k \\cdot N) where k is the number of random vectors (default 30).\n\nTODO: Implement exact O(N) trace calculation for pentadiagonal matrices to match R’s performance for exact EDF.","type":"content","url":"/compare-r#performance-note","position":29},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension"},"type":"lvl1","url":"/compare-cpp","position":0},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension"},"content":"This document compares the computational performance of the pure Python implementation (SplineFitterPy) and the C++ optimized implementation (SplineFitter) of the smoothing spline fitter.","type":"content","url":"/compare-cpp","position":1},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl2":"Setup"},"type":"lvl2","url":"/compare-cpp#setup","position":2},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl2":"Setup"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport sys\nimport os\n\n# Ensure we can import from tests\nsys.path.append(os.path.abspath('..'))\n\nfrom smoothing_spline.fitter import SplineFitter\ntry:\n    from tests.spline_fitter import SplineFitter as SplineFitterPy\nexcept ImportError:\n    print(\"Could not import pure Python SplineFitter from tests.\")\n    SplineFitterPy = None\n\n# Check if C++ extension is available\ntry:\n    from smoothing_spline._spline_extension import SplineFitterCpp as _Ext\n    CPP_AVAILABLE = True\n    print(\"C++ Extension is AVAILABLE\")\nexcept ImportError:\n    CPP_AVAILABLE = False\n    print(\"C++ Extension is NOT available\")\n\n\n\n","type":"content","url":"/compare-cpp#setup","position":3},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl2":"Speed Comparison at Different Scales"},"type":"lvl2","url":"/compare-cpp#speed-comparison-at-different-scales","position":4},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl2":"Speed Comparison at Different Scales"},"content":"We will measure the time taken to fit the smoothing spline for different numbers of observations (N) and different numbers of knots (K).\n\ndef benchmark_fitters(ns, n_knots=None):\n    results = {'py': [], 'cpp': []}\n    \n    for n in ns:\n        rng = np.random.default_rng(0)\n        x = np.sort(rng.uniform(0, 10, n))\n        y = np.sin(x) + rng.normal(0, 0.1, n)\n        \n        # Pure Python\n        if SplineFitterPy:\n            start = time.time()\n            fitter_py = SplineFitterPy(x, n_knots=n_knots, df=10)\n            fitter_py.fit(y)\n            results['py'].append(time.time() - start)\n        else:\n            results['py'].append(np.nan)\n        \n        # C++ Extension (Main Class)\n        if CPP_AVAILABLE:\n            start = time.time()\n            fitter_cpp = SplineFitter(x, n_knots=n_knots, df=10)\n            fitter_cpp.fit(y)\n            results['cpp'].append(time.time() - start)\n        else:\n            results['cpp'].append(np.nan)\n            \n    return results\n\n# Sizes to test\nns = [100, 500, 1000, 2000, 5000]\nprint(f\"Benchmarking with ns={ns}...\")\nresults_all_knots = benchmark_fitters(ns)\n\n\n\n","type":"content","url":"/compare-cpp#speed-comparison-at-different-scales","position":5},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl2":"Results Visualization"},"type":"lvl2","url":"/compare-cpp#results-visualization","position":6},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl2":"Results Visualization"},"content":"","type":"content","url":"/compare-cpp#results-visualization","position":7},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl3":"1. All Unique X as Knots","lvl2":"Results Visualization"},"type":"lvl3","url":"/compare-cpp#id-1-all-unique-x-as-knots","position":8},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl3":"1. All Unique X as Knots","lvl2":"Results Visualization"},"content":"When N is small, using all unique x values as knots is feasible. However, the complexity is O(K^3) where K is the number of knots. In this case K=N.\n\nfig, ax = plt.subplots(figsize=(10, 6))\nif SplineFitterPy:\n    ax.plot(ns, results_all_knots['py'], 'o-', label='Pure Python (SplineFitterPy)')\nif CPP_AVAILABLE:\n    ax.plot(ns, results_all_knots['cpp'], 's-', label='C++ Extension (SplineFitter)')\n\nax.set_xlabel('Number of observations (N)')\nax.set_ylabel('Time (seconds)')\nax.set_title('Speed Comparison (K = N)')\nax.legend()\nax.grid(True)\nplt.show()\n\nif CPP_AVAILABLE and SplineFitterPy:\n    speedup = [p/c for p, c in zip(results_all_knots['py'], results_all_knots['cpp'])]\n    for n, s in zip(ns, speedup):\n        print(f\"N={n:4d}: Speedup = {s:.2f}x\")\n\n\n\n\n\n","type":"content","url":"/compare-cpp#id-1-all-unique-x-as-knots","position":9},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl3":"2. Fixed Number of Knots (K=200)","lvl2":"Results Visualization"},"type":"lvl3","url":"/compare-cpp#id-2-fixed-number-of-knots-k-200","position":10},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl3":"2. Fixed Number of Knots (K=200)","lvl2":"Results Visualization"},"content":"In practice, for large N, we often limit the number of knots to a fixed K \\ll N.\n\nns_large = [1000, 5000, 10000, 20000, 50000]\nK = 200\nprint(f\"Benchmarking with large N and K={K}...\")\nresults_fixed_knots = benchmark_fitters(ns_large, n_knots=K)\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nif SplineFitterPy:\n    ax.plot(ns_large, results_fixed_knots['py'], 'o-', label=f'Pure Python (K={K})')\nif CPP_AVAILABLE:\n    ax.plot(ns_large, results_fixed_knots['cpp'], 's-', label=f'C++ Extension (K={K})')\n\nax.set_xlabel('Number of observations (N)')\nax.set_ylabel('Time (seconds)')\nax.set_title(f'Speed Comparison (Fixed K={K})')\nax.legend()\nax.grid(True)\nplt.show()\n\nif CPP_AVAILABLE and SplineFitterPy:\n    speedup_fixed = [p/c for p, c in zip(results_fixed_knots['py'], results_fixed_knots['cpp'])]\n    for n, s in zip(ns_large, speedup_fixed):\n        print(f\"N={n:5d}: Speedup = {s:.2f}x\")\n\n\n\n\n\n","type":"content","url":"/compare-cpp#id-2-fixed-number-of-knots-k-200","position":11},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl2":"GCV Solve Performance"},"type":"lvl2","url":"/compare-cpp#gcv-solve-performance","position":12},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl2":"GCV Solve Performance"},"content":"Automatic tuning with GCV involves multiple fits (or an optimized path). The C++ extension provides a highly optimized solve_gcv method.\n\nif CPP_AVAILABLE:\n    n = 5000\n    K = 200\n    rng = np.random.default_rng(0)\n    x = np.sort(rng.uniform(0, 10, n))\n    y = np.sin(x) + rng.normal(0, 0.1, n)\n    \n    print(f\"Benchmarking solve_gcv (N={n}, K={K})...\")\n    start = time.time()\n    fitter = SplineFitter(x, n_knots=K)\n    best_lam = fitter.solve_gcv(y)\n    cpp_gcv_time = time.time() - start\n    print(f\"C++ GCV solve time: {cpp_gcv_time:.4f} seconds (best lambda: {best_lam:.4e})\")\nelse:\n    print(\"C++ extension not available for GCV benchmark.\")\n\n\n\n","type":"content","url":"/compare-cpp#gcv-solve-performance","position":13},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl2":"Conclusion"},"type":"lvl2","url":"/compare-cpp#conclusion","position":14},{"hierarchy":{"lvl1":"Performance Comparison: Pure Python vs C++ Extension","lvl2":"Conclusion"},"content":"The C++ extension provides significant speedups, especially as the number of knots or observations increases. This is due to the efficient matrix operations and optimized algorithms implemented in C++ using the Eigen library.","type":"content","url":"/compare-cpp#conclusion","position":15},{"hierarchy":{"lvl1":"Boundary Behavior Comparison: smoothing_spline vs scipy"},"type":"lvl1","url":"/compare-scipy","position":0},{"hierarchy":{"lvl1":"Boundary Behavior Comparison: smoothing_spline vs scipy"},"content":"This document illustrates the differences in boundary behavior and extrapolation between smoothing_spline.SplineFitter and scipy.interpolate.make_smoothing_spline.\n\nWhile both methods fit smoothing splines, smoothing_spline explicitly implements natural cubic splines, which implies that the function should be linear beyond the boundary knots (i.e., the second derivative is zero at the boundaries).\n\nWe will demonstrate this by fitting both models to a dataset and observing their predictions outside the range of the training data.","type":"content","url":"/compare-scipy","position":1},{"hierarchy":{"lvl1":"Boundary Behavior Comparison: smoothing_spline vs scipy","lvl2":"Setup"},"type":"lvl2","url":"/compare-scipy#setup","position":2},{"hierarchy":{"lvl1":"Boundary Behavior Comparison: smoothing_spline vs scipy","lvl2":"Setup"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import make_smoothing_spline\nfrom smoothing_spline import SplineFitter\n\n# Generate synthetic data\nrng = np.random.default_rng(42)\nn = 30\nx = np.sort(rng.uniform(0, 10, n))\ny = np.sin(x) + rng.normal(0, 0.2, n)\n\n# Define extrapolation range (+/- 50% of data range)\nx_range = x.max() - x.min()\nx_plot = np.linspace(x.min() - 0.5 * x_range, x.max() + 0.5 * x_range, 200)\n\n\n\n","type":"content","url":"/compare-scipy#setup","position":3},{"hierarchy":{"lvl1":"Boundary Behavior Comparison: smoothing_spline vs scipy","lvl2":"Fitting the Models"},"type":"lvl2","url":"/compare-scipy#fitting-the-models","position":4},{"hierarchy":{"lvl1":"Boundary Behavior Comparison: smoothing_spline vs scipy","lvl2":"Fitting the Models"},"content":"We will fit both models using a similar regularization strength. Note that the parameterization of \\lambda might differ slightly, but we will try to match the degrees of freedom or visual smoothness for a fair qualitative comparison. Here we fix lam for make_smoothing_spline and use solve_gcv or fixed lambda for SplineFitter.\n\nFor simplicity, let’s fix a lambda value that provides a reasonable smooth fit.\n\n# Fit smoothing_spline\n# We explicitly ask for a specific lambda or let GCV find one.\n# Let's use GCV for smoothing_spline to get a good fit first.\nfitter = SplineFitter(x)\nlam_best = fitter.solve_gcv(y)\ny_ours = fitter.predict(x_plot)\n\n# Fit scipy.interpolate.make_smoothing_spline\n# scipy's lam is roughly related. We'll try to use a similar value or just pick one that looks good.\n# SciPy documentation says minimizes \\sum w_i (y_i - g(x_i))^2 + lam \\int g''(t)^2 dt\n# Our objective is essentially the same.\n# Note: SplineFitter scales x internally to [0, range]. Scipy does not?\n# Let's just try to match them or show the qualitative difference.\n# We'll calculate the lambda that scipy would use?\n# Let's just fit scipy with a specific lambda to ensure smoothness.\nlam_scipy = 1e-2 # heuristic\nspl_scipy = make_smoothing_spline(x, y, lam=lam_scipy)\ny_scipy = spl_scipy(x_plot)\n\n\n\n","type":"content","url":"/compare-scipy#fitting-the-models","position":5},{"hierarchy":{"lvl1":"Boundary Behavior Comparison: smoothing_spline vs scipy","lvl2":"Visualization"},"type":"lvl2","url":"/compare-scipy#visualization","position":6},{"hierarchy":{"lvl1":"Boundary Behavior Comparison: smoothing_spline vs scipy","lvl2":"Visualization"},"content":"Now we plot the data, the fitted curves within the data range, and the extrapolation.\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot Data\nax.scatter(x, y, color='black', label='Data', zorder=5)\n\n# Plot smoothing_spline result\nax.plot(x_plot, y_ours, 'b-', linewidth=2, label='smoothing_spline (Natural)')\n\n# Plot scipy result\nax.plot(x_plot, y_scipy, 'r--', linewidth=2, label='scipy.interpolate.make_smoothing_spline')\n\n# Highlight data boundaries\nax.axvline(x.min(), color='gray', linestyle=':', alpha=0.5)\nax.axvline(x.max(), color='gray', linestyle=':', alpha=0.5)\n\nax.set_title('Extrapolation Behavior: Natural vs. SciPy Spline')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_ylim([-10,10])\n\n\n\n\n\n","type":"content","url":"/compare-scipy#visualization","position":7},{"hierarchy":{"lvl1":"Boundary Behavior Comparison: smoothing_spline vs scipy","lvl2":"Analysis"},"type":"lvl2","url":"/compare-scipy#analysis","position":8},{"hierarchy":{"lvl1":"Boundary Behavior Comparison: smoothing_spline vs scipy","lvl2":"Analysis"},"content":"As seen in the plot:\n\nsmoothing_spline: The blue line becomes perfectly linear outside the vertical dashed lines (the data boundaries). This is the defining property of a natural cubic spline. The second derivative is zero at the boundary knots, and this zero curvature is maintained during extrapolation.\n\nscipy.interpolate.make_smoothing_spline: The red dashed line usually exhibits cubic extrapolation (it curves away). While make_smoothing_spline solves the smoothing spline objective, the returned B-spline object often extrapolates based on the polynomial form of the end segments, which is not necessarily constrained to be linear unless specific boundary knots or coefficients are enforced. In many statistical contexts, the “natural” boundary condition (linear extrapolation) is preferred to avoid wild oscillations outside the data range.\n\nThis highlights a key feature of the smoothing_spline package: it guarantees safe, linear extrapolation by design.","type":"content","url":"/compare-scipy#analysis","position":9},{"hierarchy":{"lvl1":"Smoothing Spline Documentation"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Smoothing Spline Documentation"},"content":"This repository provides a minimal implementation of a smoothing spline, similar to smooth.spline in R.\n\nIt is implemented in C++ with python bindings provided by pybind11.\n\nSee the table of contents for more details on the theory and comparisons with other implementations.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation"},"type":"lvl1","url":"/theory","position":0},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation"},"content":"This document outlines the theoretical background for the smoothing_spline package, synthesizing concepts from The Elements of Statistical Learning (Hastie, Tibshirani, Friedman), specifically Chapter 5. It also discusses the implementation details (Reinsch form vs. Basis form) and compares the features with R’s smooth.spline.","type":"content","url":"/theory","position":1},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl2":"1. The Smoothing Spline Problem"},"type":"lvl2","url":"/theory#id-1-the-smoothing-spline-problem","position":2},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl2":"1. The Smoothing Spline Problem"},"content":"The goal of smoothing splines is to find a function f(x) that fits the data \\{(x_i, y_i)\\}_{i=1}^N well while remaining smooth. This is formulated as the minimization of a penalized residual sum of squares:\\min_{f} \\sum_{i=1}^N (y_i - f(x_i))^2 + \\lambda \\int [f''(t)]^2 dt\n\nwhere \\lambda \\ge 0 is a smoothing parameter.\n\n\\lambda = 0: f becomes an interpolating spline (passes through every point).\n\n\\lambda \\to \\infty: f approaches the linear least squares fit.","type":"content","url":"/theory#id-1-the-smoothing-spline-problem","position":3},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl3":"The Solution: Natural Cubic Spline","lvl2":"1. The Smoothing Spline Problem"},"type":"lvl3","url":"/theory#the-solution-natural-cubic-spline","position":4},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl3":"The Solution: Natural Cubic Spline","lvl2":"1. The Smoothing Spline Problem"},"content":"Remarkably, the solution to this infinite-dimensional optimization problem is a finite-dimensional Natural Cubic Spline with knots at the unique values of x_i.\n\nA natural cubic spline is a piecewise cubic polynomial that is continuous up to its second derivative, and linear beyond the boundary knots (i.e., its second derivative is zero at the boundaries).","type":"content","url":"/theory#the-solution-natural-cubic-spline","position":5},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl2":"2. Matrix Formulation"},"type":"lvl2","url":"/theory#id-2-matrix-formulation","position":6},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl2":"2. Matrix Formulation"},"content":"Since the solution is a linear combination of basis functions, we can write:f(x) = \\sum_{j=1}^N \\theta_j N_j(x)\n\nwhere N_j(x) are the natural cubic spline basis functions.\n\nThe optimization problem reduces to:\\min_{ \\theta} (y - N \\theta)^T (y - N \\theta) + \\lambda \\theta^T \\Omega_N \\theta\n\nwhere:\n\nN_{ij} = N_j(x_i) is the basis matrix.\n\n(\\Omega_N)_{jk} = \\int N_j''(t) N_k''(t) dt is the penalty matrix.\n\nThe solution is a generalized ridge regression:\\hat{\theta} = (N^T N + \\lambda \\Omega_N)^{-1} N^T y","type":"content","url":"/theory#id-2-matrix-formulation","position":7},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl3":"Reinsch Form vs. Basis Form","lvl2":"2. Matrix Formulation"},"type":"lvl3","url":"/theory#reinsch-form-vs-basis-form","position":8},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl3":"Reinsch Form vs. Basis Form","lvl2":"2. Matrix Formulation"},"content":"There are two primary ways to compute the solution.","type":"content","url":"/theory#reinsch-form-vs-basis-form","position":9},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl4":"A. The Basis Form (Used in this Package and for K=N)","lvl3":"Reinsch Form vs. Basis Form","lvl2":"2. Matrix Formulation"},"type":"lvl4","url":"/theory#a-the-basis-form-used-in-this-package-and-for-k-n","position":10},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl4":"A. The Basis Form (Used in this Package and for K=N)","lvl3":"Reinsch Form vs. Basis Form","lvl2":"2. Matrix Formulation"},"content":"We explicitly construct the basis matrix N and the penalty matrix \\Omega.\n\nWe use the property that a natural cubic spline is determined by its values and second derivatives at the knots.\n\nOur C++ implementation constructs N efficiently by solving the tridiagonal system that relates second derivatives to values.\n\nWe then solve the dense linear system (N^T N + \\lambda \\Omega)\\theta = N^T y.\n\nWhy Basis Form?\n\nAllows for Regression Splines: We can choose fewer knots than data points (K < N). The math remains identical, but the matrices are smaller (K \t\\times K instead of N \\times N).\n\nEasier to extend to weighted least squares or other loss functions.","type":"content","url":"/theory#a-the-basis-form-used-in-this-package-and-for-k-n","position":11},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl4":"B. The Reinsch Form (Used in R’s smooth.spline)","lvl3":"Reinsch Form vs. Basis Form","lvl2":"2. Matrix Formulation"},"type":"lvl4","url":"/theory#b-the-reinsch-form-used-in-rs-smooth-spline","position":12},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl4":"B. The Reinsch Form (Used in R’s smooth.spline)","lvl3":"Reinsch Form vs. Basis Form","lvl2":"2. Matrix Formulation"},"content":"If knots are placed at every data point (K=N), the solution vector \\mathbf{f} = (f(x_1), \\dots, f(x_N))^T can be found directly without forming N.\\hat{\\mathbf{f}} = (I + \\lambda K)^{-1} y\n\nwhere K = Q R^{-1} Q^T.\n\nQ is an N \\times (N-2) tridiagonal matrix of second differences.\n\nR is an (N-2) \\times (N-2) tridiagonal matrix.\n\nThese matrices can be constructed in Python as follows:import numpy as np\nfrom scipy import sparse\n\ndef get_qr_matrices(knots):\n    n_k = len(knots)\n    hk = np.diff(knots)\n    inv_hk = 1.0 / hk\n    \n    # R is a symmetric tridiagonal matrix\n    R_k = sparse.diags(\n        [hk[1:-1] / 6.0, (hk[:-1] + hk[1:]) / 3.0, hk[1:-1] / 6.0],\n        [-1, 0, 1],\n        shape=(n_k - 2, n_k - 2),\n    )\n    \n    # Q is a tridiagonal matrix with 3 diagonals\n    Q_k = sparse.diags(\n        [inv_hk[:-1], -inv_hk[:-1] - inv_hk[1:], inv_hk[1:]],\n        [0, -1, -2],\n        shape=(n_k, n_k - 2),\n    )\n    return Q_k, R_k\n\nThis allows solving the system in O(N) time using banded solvers, making it extremely fast for large N.\n\nOur Implementation Note:\nWhile we use the basis form, our construction of \\Omega utilizes the exact same Q R^{-1} Q^T structure (computed via sparse Cholesky) to ensure mathematical equivalence to the integral penalty.","type":"content","url":"/theory#b-the-reinsch-form-used-in-rs-smooth-spline","position":13},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl2":"3. Degrees of Freedom (df)"},"type":"lvl2","url":"/theory#id-3-degrees-of-freedom-df","position":14},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl2":"3. Degrees of Freedom (df)"},"content":"The smoothing parameter \\lambda is abstract. A more intuitive measure of model complexity is the effective degrees of freedom (df).df(\\lambda) = \\text{trace}(\\mathbf{S}_\\lambda)\n\nwhere \\mathbf{S}_\\lambda is the smoother matrix such that \\hat{y} = \\mathbf{S}_\\lambda y.\nIn our formulation:\\mathbf{S}_\\lambda = N (N^T N + \\lambda \\Omega)^{-1} N^T\n\nThe relationship between df and \\lambda is monotonic.\n\ndf \\to 2 as \\lambda \\to \\infty (Linear fit).\n\ndf  \\to N as \\lambda \\to 0 (Interpolation).\n\nOur package allows specifying df directly. We use a root-finding algorithm (Brent’s method) to find the unique \\lambda that yields the requested df.","type":"content","url":"/theory#id-3-degrees-of-freedom-df","position":15},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl2":"4. Comparison with R’s smooth.spline"},"type":"lvl2","url":"/theory#id-4-comparison-with-rs-smooth-spline","position":16},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl2":"4. Comparison with R’s smooth.spline"},"content":"Feature\n\nsmooth.spline (R)\n\nsmoothing_spline (Python)\n\nAlgorithm\n\nReinsch Form (O(N))\n\nBasis Form (O(NK^2) or O(K^3))\n\nKnots\n\nAll unique x (default) or nknots\n\nAll unique x or specified n_knots\n\nInput \\lambda\n\nVia spar or lambda\n\nVia lamval\n\nInput df\n\nSupported\n\nSupported (via root finding)\n\nAutomatic Tuning\n\nGCV / CV\n\nGCV\n\nWeights\n\nSupported\n\nSupported\n\nDerivatives\n\nBuilt-in\n\nExposed via predict(deriv=...)\n\nExtrapolation\n\nLinear (via predict)\n\nLinear (explicitly handled)","type":"content","url":"/theory#id-4-comparison-with-rs-smooth-spline","position":17},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl3":"Key Differences","lvl2":"4. Comparison with R’s smooth.spline"},"type":"lvl3","url":"/theory#key-differences","position":18},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl3":"Key Differences","lvl2":"4. Comparison with R’s smooth.spline"},"content":"Speed: For N=10,000, smooth.spline is faster because it exploits the band structure of the full system. Our implementation allows n_knots < N (Regression Splines), which restores speed (O(N K^2)) and reduces overfitting risk, a feature smooth.spline supports via nknots.\n\nSpar (R specific): R uses a scaling parameter spar related to \\lambda. We use raw \\lambda (scaled by x-range cubed for numerical stability).","type":"content","url":"/theory#key-differences","position":19},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl2":"5. References"},"type":"lvl2","url":"/theory#id-5-references","position":20},{"hierarchy":{"lvl1":"Smoothing Splines: Theory and Implementation","lvl2":"5. References"},"content":"Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd Edition. Springer. (Chapter 5).\n\nGreen, P. J., & Silverman, B. W. (1994). Nonparametric Regression and Generalized Linear Models: A Roughness Penalty Approach. Chapman and Hall/CRC.","type":"content","url":"/theory#id-5-references","position":21}]}